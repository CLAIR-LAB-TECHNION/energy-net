{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB1do7jbvPK-"
      },
      "source": [
        "# EnergyNet Tutorial\n",
        "\n",
        "Welcome to **EnergyNet**! This notebook will guide you through installing, setting up, and using this Python package for **smart grid simulation**.  \n",
        "\n",
        "**Author:** Michael Wein  \n",
        "**GitHub:** [energy-net](https://github.com/CLAIR-LAB-TECHNION/energy-net)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkHy0DCM-hza"
      },
      "source": [
        "#Setting up\n",
        "Let's import the GitHub repository we’ll be working with: energy-net! The version we’re using here is publicly accessible and will only be updated with the changes from your branch once we’re certain everything is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G2nSwTg4K1d"
      },
      "outputs": [],
      "source": [
        "%rm -rf energy-net # In case you have run this notebook in the past, and have an older version of energy-net\n",
        "!git clone https://github.com/CLAIR-LAB-TECHNION/energy-net.git\n",
        "%cd energy-net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWAM8C8Pqhz8"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T3qM2GY9ph-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\") #To ignore warnings about your Python version or other silly things that clutter up the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANEcfkY_UGJ"
      },
      "source": [
        "# PCS Environment Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkR9scpzl9Pd"
      },
      "source": [
        "\n",
        "The `PCSEnv` class, located in the `pcs_env.py` file, is a **custom Gym environment** designed to simulate the behavior of a **PCS unit**. A PCS unit is composed of three types of components, which is what the name comes from:\n",
        "\n",
        "- **Production units**: Components that generate energy (e.g., generators, solar panels, turbines).  \n",
        "- **Consumption units**: Components that consume energy (e.g., electrical loads, machinery, appliances).  \n",
        "- **Storage devices**: Components that store energy for later use (e.g., batteries, thermal storage tanks, capacitors).  \n",
        "\n",
        "While the PCS unit can hold any number of units of each type, this environment is simplified to include **only a single consumption unit and a single storage unit** (with no production units).  \n",
        "\n",
        "Now, you might be thinking: *“If there’s no production unit, where does the energy come from?”* Great question! In this environment, the PCS unit **buys energy from the grid (based on a set price curve)**, essentially an outside seller that can supply unlimited energy throughout the day at a variable price.  \n",
        "\n",
        "This “price curve” will become more important later when we introduce an ISO (an entity that buys and sells energy). For now, it’s part of the environment dynamics and **cannot be controlled by the agent**.  \n",
        "\n",
        "It’s also worth noting that the PCS unit **can sell energy back** to the grid. For simplicity, buying and selling prices are the same in this environment. The **only control the agent has** is deciding how much energy to buy or sell at each timestep.  \n",
        "\n",
        "## Agent Goal / Reward\n",
        "\n",
        "The agent’s performance is evaluated based on two main criteria:\n",
        "\n",
        "- **Money**: The agent earns or loses money depending on when it buys and sells energy. Ideally, it should buy when prices are low and sell when prices are high to maximize profit over the day.  \n",
        "- **Meeting energy demand**: The agent must provide enough energy for the consumption unit at every timestep. If it falls short, it receives a configurable negative reward.  \n",
        "\n",
        "To make the task easier, the environment provides **predicted consumption data for the next timestep**, generated by an external model in `energy_net/consumption_prediction`.  \n",
        "\n",
        "Let's start with the actual code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYj_TTx195Is"
      },
      "source": [
        "##Setting up a basic environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8EMnkUBVNP"
      },
      "source": [
        "First, we create the environment. Here, we have the following parameters (we will cover more later):\n",
        "- **render_mode**: for if you want the agent to (be able to) call env.render(), which provides information about what is happening at each time step. This is helpful for understanding what's happening in an episode, so we will use it for this simulation. In order to render, write \"render_mode = 'human'\" and write env.render() at each time step.\n",
        "- **Data and prediction file path**: The prediction file predicts how much the consumption unit will consume at each step; the data file is how much is actually consumed.\n",
        "- **Shortage Penalty**: this is how much the agent is punished for not having enough energy for a single time step. It should be chosen in accordance with the price for buying/selling energy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaS772N4BQTq"
      },
      "outputs": [],
      "source": [
        "from energy_net.gym_envs.pcs_env import *\n",
        "env = PCSEnv(render_mode='human', test_data_file='tests/gym/data_for_tests/synthetic_household_consumption_test.csv',\n",
        "                 predictions_file='tests/gym/data_for_tests/consumption_predictions_without_features.csv',shortage_penalty = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQJfvf1DKcFx"
      },
      "source": [
        "Here, and for the rest of this tutorial, we're going to treat one episode as one day where each interval or state is one half hour (in total, 48 states per day). It is possible to configure this fairly easily to be different (for example, for intervals to be much smaller or larger, and for episodes to be more than one day) using the environment parameters dt and episode_length_days, but for the purposes of this tutorial, we will use the default.\n",
        "\n",
        "Every time we call env.reset(), it advances to the next day's data - if we hit the end of a data file, it will loop back to the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbXPqQaIxoZk"
      },
      "outputs": [],
      "source": [
        "# Reset at the start of each day.\n",
        "# This clears financial metrics and ensures the strategy starts fresh for the date.\n",
        "obs, info = env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M69IaSxhsXd"
      },
      "source": [
        "Here, we're going to take on a simple strategy - buy 10 units of energy for the first 24 time steps, and then sell 5 for the next 24 time steps.\n",
        "\n",
        "\n",
        "*   Note - this will be restrained by how much it is actually possible to buy/sell. For example, if the PCS unit tries to buy 10 units of energy while energy is ≥ 90 (because 100 is the default max capacity here), then at the next timestep there will be 100 units in storage. Similarly, when it tries to sell more energy than it has, the environmnet will just sell the rest of the energy it does have and no more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH8Qg0iYdJBQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Date: {env.current_datetime.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "day_reward = 0.0\n",
        "time_step = 0\n",
        "\n",
        "\n",
        "#Intra-day Time Steps (48 steps per day, one for each half hour)\n",
        "while True:\n",
        "  # Generate a random battery action (Intent) between -10 and 10.\n",
        "  # -10: Maximum Charge, 10: Maximum Discharge.\n",
        "  action_value = 10 if time_step < 24 else -5\n",
        "  action = np.array([action_value], dtype=np.float32)\n",
        "\n",
        "  # Step the simulation.\n",
        "  # The env will internally query the PriceCurveStrategy for the current price,\n",
        "  # calculate the financial reward, and update the battery/consumption state.\n",
        "  obs, reward, terminated, truncated, info = env.step(action)\n",
        "  #Render all the details of the current time slot\n",
        "  env.render()\n",
        "\n",
        "  # Accumulate rewards (Net financial performance).\n",
        "  day_reward += reward\n",
        "\n",
        "  print(f\"Step Reward: {reward:.2f}\")\n",
        "\n",
        "  # Check for End of Day\n",
        "  # Terminated becomes True once the current_step reaches max_steps (48).\n",
        "  if terminated or truncated:\n",
        "      print(f\"\\n>>> Day Finished!\")\n",
        "      print(f\">>> Total Day Reward: {day_reward:.2f}\")\n",
        "      print(f\">>> Final Storage: {info['storage_after_units']:.2f} units\")\n",
        "      print(f\">>> Total Shortages Today: {env.shortage_count}\")\n",
        "      break\n",
        "  time_step+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZxeWa3sQpCC"
      },
      "source": [
        "## State Variables\n",
        "\n",
        "- **Storage**  \n",
        "  Energy stored in the battery at timestep t.\n",
        "\n",
        "- **Consumption (Actual)**  \n",
        "  Energy deducted during the transition from timestep t to t+1.\n",
        "\n",
        "- **Current Price**  \n",
        "  Cost of buying one unit of energy at timestep t.\n",
        "\n",
        "- **Last Agent Action**  \n",
        "  Energy bought or sold during timestep t-1.  \n",
        "  **Note:** This only affects timestep t.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZf7JXvFQyDE"
      },
      "source": [
        "## How the Transition Works\n",
        "\n",
        "Let’s look at a concrete example of an energy transition for better clarity.\n",
        "\n",
        "### Example: Timestep 5\n",
        "\n",
        "- Stored energy: 5 units  \n",
        "- Energy price: 0.1 per unit  \n",
        "- Consumption this timestep: 3 units  \n",
        "- Agent buys: 1 unit (cost = 1 × 0.1 = 0.1)\n",
        "\n",
        "**Step 1: Apply consumption**\n",
        "\n",
        "$$\n",
        "5 - 3 = 2 \\quad \\text{units remain in storage}\n",
        "$$\n",
        "\n",
        "**Step 2: Apply agent action (buy/sell)**\n",
        "\n",
        "$$\n",
        "2 + 1 = 3 \\quad \\text{units in storage at the start of timestep 6}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4jv0vZfQ3Es"
      },
      "source": [
        "## Important Insights\n",
        "\n",
        "- **Consumption happens before buying energy**.  The agent must plan ahead:\n",
        "  - To cover consumption at timestep t, ensure sufficient storage by timestep **t-1**.\n",
        "  - Once t starts, it is **too late** to buy energy for that timestep.\n",
        "  - Decisions are **anticipatory**, not reactive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYx_RJm6Q6Fb"
      },
      "source": [
        "- Daily Battery Reset\n",
        "\n",
        "  - In `PCSEnv`, the battery is reset to **0 units at the start of each day**. This encourages the agent to plan consumption and charging/discharging **within a single day**. No hoarding energy across days → focus on short-term optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYb1we8SRIIn"
      },
      "source": [
        "Now, let's train a real agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_2VUTCP7aoI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "from stable_baselines3 import SAC\n",
        "warnings.simplefilter(\"ignore\") #To ignore warnings about your Python version or other silly things that clutter up the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZwCRlT5K7fX0"
      },
      "outputs": [],
      "source": [
        "# ---------------- CONFIG ----------------\n",
        "TRAIN_TIMESTEPS = 100000   # lower for quick tests; raise for serious training\n",
        "EVAL_DAYS = 3              # how many days to evaluate after training\n",
        "SEED = 42\n",
        "\n",
        "# Create SAC model (device='auto' will use GPU in Colab if available)\n",
        "model = SAC(\"MlpPolicy\", env, verbose=0, seed=SEED, device=\"auto\")\n",
        "\n",
        "# Train\n",
        "print(\"Starting training... (this may take a while depending on TRAIN_TIMESTEPS)\")\n",
        "model.learn(total_timesteps=TRAIN_TIMESTEPS)\n",
        "print(\"Training finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFjU-yjRTvnl"
      },
      "source": [
        "Next, let's evaluate how this function did! The code below will evaluate how the code did and produce various graphs. Feel free to test different algorithms against each other to see what works best here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HSs0F-Co7lr-"
      },
      "outputs": [],
      "source": [
        "# ---------------- EVALUATION ----------------\n",
        "env_eval = PCSEnv(render_mode='human', test_data_file='tests/gym/data_for_tests/synthetic_household_consumption_test.csv',\n",
        "                 predictions_file='tests/gym/data_for_tests/consumption_predictions_without_features.csv',shortage_penalty = 1)\n",
        "# Prepare storage for metrics\n",
        "timestamps = []\n",
        "storage_before = []\n",
        "storage_after = []\n",
        "prices = []\n",
        "actions = []\n",
        "step_money = []\n",
        "total_money = []\n",
        "shortages = []\n",
        "consumptions = []\n",
        "avail_discharges = []\n",
        "\n",
        "num_days = EVAL_DAYS\n",
        "steps_per_day = env_eval.max_steps  # usually 48 for dt=0.5/24\n",
        "\n",
        "for day in range(num_days):\n",
        "    # reset returns (obs, info) in Gymnasium\n",
        "    obs, info = env_eval.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # predict action pass only the observation\n",
        "        action, _state = model.predict(obs, deterministic=True)\n",
        "\n",
        "        # Step env and unpack Gymnasium outputs\n",
        "        next_obs, reward, terminated, truncated, info = env_eval.step(action)\n",
        "        done = bool(terminated or truncated)\n",
        "\n",
        "        # timestamp corresponding to the step (env.current_datetime has already advanced)\n",
        "        ts = env_eval.current_datetime - timedelta(days=env_eval.dt)\n",
        "\n",
        "        # Log metrics\n",
        "        timestamps.append(ts)\n",
        "        storage_before.append(info.get(\"storage_before_units\"))\n",
        "        storage_after.append(info.get(\"storage_after_units\"))\n",
        "        prices.append(info.get(\"current_price\"))\n",
        "        actions.append(float(info.get(\"battery_action\", action[0] if hasattr(action, \"__len__\") else float(action))))\n",
        "        step_money.append(info.get(\"step_money\", 0.0))\n",
        "        total_money.append(info.get(\"total_money_so_far\", env_eval.get_money()))\n",
        "        shortages.append(1 if info.get(\"shortage\", False) else 0)\n",
        "        consumptions.append(info.get(\"consumption_units\", 0.0))\n",
        "        avail_discharges.append(info.get(\"available_discharge_units\", 0.0))\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "print(\"Evaluation finished. Collected steps:\", len(timestamps))\n",
        "\n",
        "# Put into DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"timestamp\": timestamps,\n",
        "    \"storage_before\": storage_before,\n",
        "    \"storage_after\": storage_after,\n",
        "    \"price\": prices,\n",
        "    \"action\": actions,\n",
        "    \"step_money\": step_money,\n",
        "    \"total_money\": total_money,\n",
        "    \"shortage\": shortages,\n",
        "    \"consumption\": consumptions,\n",
        "    \"available_discharge\": avail_discharges\n",
        "})\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wx6KgUnO7oJg"
      },
      "outputs": [],
      "source": [
        "# ---------- SIMPLE SINGLE PLOTS ----------\n",
        "def line_plot(x, y, title, xlabel=\"timestep\", ylabel=None, figsize=(10,3)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    if ylabel:\n",
        "        plt.ylabel(ylabel)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# storage before/after\n",
        "line_plot(df['timestamp'], df['storage_before'], \"Storage (before step)\", xlabel=\"time\", ylabel=\"units\")\n",
        "line_plot(df['timestamp'], df['storage_after'],  \"Storage (after step)\",  xlabel=\"time\", ylabel=\"units\")\n",
        "\n",
        "# price\n",
        "line_plot(df['timestamp'], df['price'], \"Price over time\", xlabel=\"time\", ylabel=\"price\")\n",
        "\n",
        "# actions\n",
        "line_plot(df['timestamp'], df['action'], \"Agent Actions over time\", xlabel=\"time\", ylabel=\"battery_intent\")\n",
        "\n",
        "# money (cumulative)\n",
        "df['cumulative_money'] = df['step_money'].cumsum()\n",
        "line_plot(df['timestamp'], df['cumulative_money'], \"Cumulative Money over time\", xlabel=\"time\", ylabel=\"money\")\n",
        "\n",
        "# shortages as step plot\n",
        "plt.figure(figsize=(12,2.5))\n",
        "plt.plot(df['timestamp'], df['shortage'], drawstyle='steps-post')\n",
        "plt.title(\"Shortages (0/1) over time\")\n",
        "plt.xlabel(\"time\")\n",
        "plt.ylabel(\"shortage\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- COMBINED MULTI-PANEL SUMMARY ----------\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14,12), sharex=True)\n",
        "fig.suptitle(\"PCS Evaluation Summary\", fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0].plot(df['timestamp'], df['storage_after'])\n",
        "axes[0].set_ylabel(\"Storage (units)\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(df['timestamp'], df['price'])\n",
        "axes[1].set_ylabel(\"Price ($/unit)\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].plot(df['timestamp'], df['action'])\n",
        "axes[2].set_ylabel(\"Action (battery intent)\")\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "axes[3].plot(df['timestamp'], df['cumulative_money'])\n",
        "axes[3].set_ylabel(\"Cumulative money ($)\")\n",
        "axes[3].set_xlabel(\"time\")\n",
        "axes[3].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIJEhmY_5tH"
      },
      "source": [
        "# ISO Environment Guide\n",
        "\n",
        "## What This Code Does\n",
        "\n",
        "The `ISOEnv` is a **custom Gymnasium environment** designed for simulating electricity grid management scenarios. It represents a **day-ahead energy dispatch problem** where you need to make decisions about pricing and energy dispatch based on predicted consumption patterns.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "Imagine you're managing an electrical grid and need to:\n",
        "\n",
        "1. **Predict** how much electricity households will consume tomorrow (you have forecasts)  \n",
        "2. **Plan** how much power to dispatch and at what price  \n",
        "3. **Evaluate** your decisions against what actually happened  \n",
        "\n",
        "This environment simulates this scenario by:\n",
        "\n",
        "- Loading **actual consumption data** (what really happened)  \n",
        "- Loading **predicted consumption data** (your forecasts)  \n",
        "- Allowing an agent to make decisions (prices and dispatch amounts)  \n",
        "- Scoring performance based on how close the dispatch was to actual consumption  \n",
        "\n",
        "### Key Features\n",
        "\n",
        "**Automatic Feature Detection**: The environment automatically detects and includes any additional feature columns in your prediction CSV (like weather data, time features, etc.) beyond just the consumption forecast.\n",
        "\n",
        "**Day-Based Episodes**: Each episode represents one full day, split into 48 half-hour time slots (30 minutes each).\n",
        "\n",
        "**Observation Space**: For each day, you receive:\n",
        "\n",
        "- 48 predicted consumption values (one per time slot)  \n",
        "- Additional features for each time slot (if present in your data)  \n",
        "\n",
        "**Action Space**: You must provide:\n",
        "\n",
        "- 48 price values (what to charge per time slot)  \n",
        "- 48 dispatch values (how much power to supply per time slot)  \n",
        "\n",
        "**Performance Metric**: The environment uses **Mean Absolute Error (MAE)** between your dispatch and actual consumption as the cost function. Lower is better!\n",
        "\n",
        "## How to Use This Environment\n",
        "\n",
        "### Step 1: Prepare Your Data\n",
        "\n",
        "You need two CSV files:\n",
        "\n",
        "**Actual Consumption CSV** (`actual.csv`) and\n",
        "**Predicted Consumption CSV** (`predictions.csv`)\n",
        "Let's initialize the environment!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "avB00Iik_7HD"
      },
      "outputs": [],
      "source": [
        "from energy_net.gym_envs.iso_env import *\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "\n",
        "env = ISOEnv(\n",
        "    actual_csv='tests/gym/data_for_tests/synthetic_household_consumption_test.csv',\n",
        "    predicted_csv='tests/gym/data_for_tests/consumption_predictions_without_features.csv',\n",
        "    steps_per_day=48  # 48 half-hour slots = 24 hours\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIlp4RJdDmoK"
      },
      "source": [
        "Next, let's run a basic simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DGbNbHkGBReT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reset to get the first day's observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# The observation contains predictions + features\n",
        "# Let's extract just the predictions (first 48 values)\n",
        "predictions = obs[:48]\n",
        "\n",
        "# Create a simple action: prices and dispatch amounts\n",
        "prices = np.linspace(0.1, 0.5, 48)  # Prices from $0.10 to $0.50\n",
        "dispatch = predictions  # Dispatch exactly what we predicted\n",
        "\n",
        "# Combine into action: [48 prices, 48 dispatch values]\n",
        "action = np.concatenate([prices, dispatch])\n",
        "\n",
        "# Take a step (simulate one day)\n",
        "next_obs, reward, terminated, truncated, step_info = env.step(action)\n",
        "\n",
        "# Visualize the results\n",
        "env.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEN-lTltEUtX"
      },
      "source": [
        "### Understanding the Output\n",
        "\n",
        "When you call `env.render()`, you'll see:\n",
        "\n",
        "- **Time slots**: Each row shows a 30-minute period\n",
        "- **CSV Row**: Which row in your original data this corresponds to\n",
        "- **Pred vs Actual**: Forecasted vs actual consumption\n",
        "- **Dispatch**: How much power you decided to supply\n",
        "- **Price**: The price you set\n",
        "- **Features**: Any additional columns (weather, time features, etc.)\n",
        "- **MAE (Mean Absolute Error)**: How accurate your dispatch was\n",
        "\n",
        "\n",
        "Now, let's try training an agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VuGMPXWKETeM"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import SAC\n",
        "\n",
        "# Train an agent\n",
        "model = SAC(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    gamma=0.0,       # <-- critical\n",
        "    learning_starts=0,\n",
        "    train_freq=1,\n",
        "    gradient_steps=1000,\n",
        "    verbose=0,\n",
        ")\n",
        "model.learn(total_timesteps=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TedXaHpowsCc"
      },
      "source": [
        "Now, let's graph the output of this learning process! Don't expect such great results yet - since this ISO is acting without the input of a PCS unit consuming energy, we had to give it a fairly bogus reward. In the alternating environment, it will have a better reward function.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ATQUpkvSwJP4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the last day's data from the environment's render record\n",
        "# Run this cell after your training/testing loop\n",
        "\n",
        "# First, let's capture data during the test loop\n",
        "obs, info = env.reset()\n",
        "day_records = []\n",
        "\n",
        "for day in range(10):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    record = env.render()\n",
        "\n",
        "    # Store the record for plotting\n",
        "    if record is not None:\n",
        "        day_records.append({\n",
        "            'actual': info['realized'],\n",
        "            'dispatch': info['dispatch'],\n",
        "            'prices': info['prices'],\n",
        "            'timestamp': info['timestamp']\n",
        "        })\n",
        "\n",
        "    if terminated:\n",
        "        obs, info = env.reset()\n",
        "\n",
        "# Now plot the last day (or any specific day)\n",
        "day_to_plot = -1  # -1 for last day, 0 for first day, etc.\n",
        "\n",
        "if day_records:\n",
        "    data = day_records[day_to_plot]\n",
        "\n",
        "    # Create time steps (48 half-hour intervals)\n",
        "    timesteps = np.arange(len(data['actual']))\n",
        "\n",
        "    # Create figure with all three metrics\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "    # Plot actual consumption and dispatch on left y-axis\n",
        "    ax1.set_xlabel('Time Step (30-min intervals)', fontsize=12)\n",
        "    ax1.set_ylabel('Consumption / Dispatch (kWh)', fontsize=12, color='black')\n",
        "\n",
        "    line1 = ax1.plot(timesteps, data['actual'],\n",
        "                     color='#FF1744', linewidth=2.5,\n",
        "                     label='Actual Consumption', marker='o', markersize=4)\n",
        "    line2 = ax1.plot(timesteps, data['dispatch'],\n",
        "                     color='#00E676', linewidth=2.5,\n",
        "                     label='Model Dispatch', marker='s', markersize=4, linestyle='--')\n",
        "\n",
        "    ax1.tick_params(axis='y', labelcolor='black')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Create second y-axis for prices\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('Price ($/kWh)', fontsize=12, color='#2979FF')\n",
        "\n",
        "    line3 = ax2.plot(timesteps, data['prices'],\n",
        "                     color='#2979FF', linewidth=2.5,\n",
        "                     label='ISO Price', marker='^', markersize=4, linestyle=':')\n",
        "\n",
        "    ax2.tick_params(axis='y', labelcolor='#2979FF')\n",
        "\n",
        "    # Combine legends\n",
        "    lines = line1 + line2 + line3\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax1.legend(lines, labels, loc='upper left', fontsize=11, framealpha=0.9)\n",
        "\n",
        "    # Title with date\n",
        "    plt.title(f'ISO Environment - Day Analysis\\n{data[\"timestamp\"].strftime(\"%Y-%m-%d\")}',\n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    mae = np.mean(np.abs(data['dispatch'] - data['actual']))\n",
        "    avg_price = np.mean(data['prices'])\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Day Summary for {data['timestamp'].strftime('%Y-%m-%d')}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f} kWh\")\n",
        "    print(f\"Average Price Set:         ${avg_price:.4f}/kWh\")\n",
        "    print(f\"Total Actual Consumption:  {np.sum(data['actual']):.2f} kWh\")\n",
        "    print(f\"Total Dispatch:            {np.sum(data['dispatch']):.2f} kWh\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "else:\n",
        "    print(\"No day records available. Make sure to run the test loop first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb5C6wHYFEXG"
      },
      "source": [
        "## Important Notes on ISO Environment\n",
        "\n",
        "- **One step = One day**: Each call to `step()` processes an entire day (48 time slots)  \n",
        "- **Automatic loop**: When the environment reaches the end of your data, it automatically loops back to the beginning  \n",
        "- **Reward**: Negative MAE (so maximizing reward = minimizing error)  \n",
        "- **Sequential days**: The environment automatically advances through your dataset day by day\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GosBQ5ovHCUc"
      },
      "source": [
        "# Alternating ISO-PCS\n",
        "\n",
        "## What This Code Does\n",
        "\n",
        "Now we get to the **crux of EnergyNet - a multi agent environment**! This code implements a **two-agent energy market simulation** where an ISO (Independent System Operator) and PCS learn to interact in an alternating training framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HYO2nnIIgIF"
      },
      "source": [
        "### The Two Agents\n",
        "\n",
        "**1. ISO Agent (Grid Operator)**\n",
        "\n",
        "- **Goal**: Set electricity prices that maximize revenue while maintaining grid stability  \n",
        "- **Learns**: Optimal pricing strategies based on predicted consumption patterns  \n",
        "- **Action Space**: 96 values per day (48 prices + 48 dispatch targets)  \n",
        "- **Reward**: Money earned from electricity sales minus shortage penalties  \n",
        "\n",
        "**2. PCS Agent (Household/Battery System)**\n",
        "\n",
        "- **Goal**: Minimize electricity costs while meeting energy needs  \n",
        "- **Learns**: When to use battery storage vs. grid power based on prices  \n",
        "- **Action Space**: Battery charge/discharge decisions  \n",
        "- **Reward**: Cost savings from strategic energy management  \n",
        "\n",
        "### How They Interact\n",
        "For each iteration:\n",
        "\n",
        "1. **ISO Phase**: Train the grid operator for N days  \n",
        "   - ISO sets prices based on current policy  \n",
        "   - Observes household responses  \n",
        "   - Updates pricing strategy  \n",
        "\n",
        "2. **PCS Phase**: Train households for the next N days  \n",
        "   - Households respond to ISO's prices  \n",
        "   - Learn optimal battery strategies  \n",
        "   - Update energy management policy  \n",
        "\n",
        "This mimics real-world dynamics where grid operators and consumers continuously adapt to each other's behavior.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "- **AlternatingISOEnv**: Coordinates both agents:  \n",
        "  - Runs PCS environment for 48 half-hour steps (one day)  \n",
        "  - Collects metrics: money earned, forecast accuracy (MAE), shortages  \n",
        "  - Passes observations between the two systems  \n",
        "- **PenalizedAlternatingISOEnv**: Adds financial penalties for shortages (supply < demand) to encourage conservative dispatch decisions. The effects of this strategy have yet to be studied in this project.  \n",
        "- **RLPriceCurveStrategy**: Converts ISO actions into price curves that the PCS agent observes  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFArZ7ANIl05"
      },
      "source": [
        "Let's see an example! First, let's important the relevant code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F5oIHxeo6uXG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/energy-net/energy_net/gym_envs\")\n",
        "from energy_net.gym_envs.alternating_env import *\n",
        "from energy_net.gym_envs.evaluate_alternating import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8MuOSOm_mO8"
      },
      "source": [
        "Next, let's run a short loop.  \n",
        "\n",
        "All you need to do is call `run_alternating_training`. If you want to keep track of the training process for later visualization, you can assign the output to a variable (here, `history`): history = run_alternating_training(...)\n",
        "\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **cycle_days**: Number of days per training cycle. Determines how long each agent has to learn a policy before switching control to the other agent.  \n",
        "  *Example:* 7 → agents switch control every week.\n",
        "\n",
        "- **total_iterations**: Total number of training iterations.\n",
        "\n",
        "- **pcs_algo_cls**: Algorithm class for the PCS agent. Default: `PPO`.\n",
        "\n",
        "- **iso_algo_cls**: Algorithm class for the ISO agent. Default: `PPO`.\n",
        "\n",
        "- **pcs_policy**: Policy type for the PCS agent.\n",
        "\n",
        "- **iso_policy**: Policy type for the ISO agent.\n",
        "\n",
        "- **pcs_algo_kwargs**: Additional keyword arguments for the PCS algorithm.\n",
        "\n",
        "- **iso_algo_kwargs**: Additional keyword arguments for the ISO algorithm.\n",
        "\n",
        "- **pcs_steps_per_day**: Number of steps per day for PCS training.\n",
        "\n",
        "- **test_data_file**: Path to the test data CSV file.\n",
        "\n",
        "- **predictions_file**: Path to the predictions CSV file.\n",
        "\n",
        "- **verbose**: Verbosity level.\n",
        "\n",
        "- **render**: Whether to render the environment.\n",
        "\n",
        "- **render_every_n_steps**: Render frequency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NmoBMjCCJgBo"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(\"ignore\")\n",
        "history = run_alternating_training(\n",
        "    cycle_days=7,\n",
        "    total_iterations=30,\n",
        "    render=False,              # Show daily reports\n",
        "    render_every_n_steps=100,    # Render every 100 timesteps\n",
        "    test_data_file = 'tests/gym/data_for_tests/synthetic_household_consumption_test.csv',\n",
        "    predictions_file = 'tests/gym/data_for_tests/consumption_predictions.csv',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUObv7tdFYsI"
      },
      "source": [
        "Now, if we want to run an experiment and see visual results, we can use run_experiment for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq9LjSRmFXpO"
      },
      "outputs": [],
      "source": [
        "# Run normally\n",
        "metrics, results_df = run_experiment(\n",
        "    actual_csv=\"tests/gym/data_for_tests/synthetic_household_consumption_test.csv\",\n",
        "    pred_csv=\"tests/gym/data_for_tests/consumption_predictions.csv\",\n",
        "    iterations=90,\n",
        "    num_days=21,\n",
        "    out_dir=\"/content/results\",\n",
        "    run_id=\"colab_test\",\n",
        ")\n",
        "\n",
        "# Then display saved files\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "files = sorted(glob.glob(\"/content/results/*.png\"))\n",
        "print(\"Found images:\", files)\n",
        "for f in files:\n",
        "    display(Image(filename=f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s56b4tILGLt"
      },
      "source": [
        "### Challenges\n",
        "- Achieving convergence has proven difficult. While the PCS agent’s policy tends to converge reasonably well in isolation, the simultaneous adaptation of the ISO agent appears to disrupt this process.  \n",
        "- Establishing an effective reward-sharing mechanism between the two agents. Although we experimented with an environment designed to encourage collaboration, the impact of this approach has not yet been fully evaluated.  \n",
        "- The PCS agent exhibits a consistent reluctance to purchase substantial amounts of energy in these scenarios, which may prevent short-term financial losses but ultimately exacerbates its situation by generating significant shortages.\n",
        "### Potential Approaches for Optimization\n",
        "- Enhance the ISO agent’s ability to leverage the provided prediction data more effectively.  \n",
        "- Allow additional training time for both agents to facilitate better policy development.  \n",
        "- Experiment with alternative learning algorithms to improve convergence and coordination.  \n",
        "- Test different cycle lengths to provide each agent with sufficient time to adapt.  \n",
        "- Develop and implement a more effective shared reward structure to encourage collaboration between PCS and ISO.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}